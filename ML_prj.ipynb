{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91bc8d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from conllu import parse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence, pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from conllu import parse_incr\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b5f150",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sentences collected: 3323\n"
     ]
    }
   ],
   "source": [
    "# Tổng hợp dữ liệu từ file .conllu\n",
    "def load_conllu_data(data_path):\n",
    "    all_sentences = []\n",
    "    for file_name in os.listdir(data_path):\n",
    "        if file_name.endswith('.conllu'):\n",
    "            with open(os.path.join(data_path, file_name), 'r', encoding='utf-8') as file:\n",
    "                for sentence in parse_incr(file):\n",
    "                    all_sentences.append(sentence)\n",
    "    return all_sentences\n",
    "\n",
    "data_path = './data/UD_Vietnamese-VTB'\n",
    "all_sentences = load_conllu_data(data_path)\n",
    "\n",
    "print(f\"Total sentences collected: {len(all_sentences)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4522328c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 7488, Tag set size: 17\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(sentences):\n",
    "    word_vocab = defaultdict(lambda: len(word_vocab))\n",
    "    tag_vocab = defaultdict(lambda: len(tag_vocab))\n",
    "\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            word = token['form']\n",
    "            tag = token['upostag']\n",
    "            word_vocab[word]\n",
    "            tag_vocab[tag]\n",
    "\n",
    "    # Đóng băng từ vựng để tránh thêm từ mới\n",
    "    word_vocab.default_factory = None\n",
    "    tag_vocab.default_factory = None\n",
    "\n",
    "    return word_vocab, tag_vocab\n",
    "\n",
    "word_vocab, tag_vocab = build_vocab(all_sentences)\n",
    "\n",
    "print(f\"Vocabulary size: {len(word_vocab)}, Tag set size: {len(tag_vocab)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2b87111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_tensor(sentence, word_vocab, tag_vocab):\n",
    "    words = [word_vocab[token['form']] for token in sentence]\n",
    "    tags = [tag_vocab[token['upostag']] for token in sentence]\n",
    "    return torch.tensor(words, dtype=torch.long), torch.tensor(tags, dtype=torch.long)\n",
    "\n",
    "sentences_tensors = [sentence_to_tensor(sentence, word_vocab, tag_vocab) for sentence in all_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cb4e3d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 2658, Testing data size: 665\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = train_test_split(sentences_tensors, test_size=0.2, random_state=26)\n",
    "\n",
    "print(f\"Training data size: {len(train_data)}, Testing data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a764c6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DependencyParsingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "train_dataset = DependencyParsingDataset(train_data)\n",
    "test_dataset = DependencyParsingDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83ff6724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\phanb\\AppData\\Roaming\\Python\\Python39\\site-packages\\transformers\\utils\\generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.6013954225040616\n",
      "Epoch 2, Loss: 0.885767271121343\n",
      "Epoch 3, Loss: 0.6497937753086999\n",
      "Epoch 4, Loss: 0.4961752827678408\n",
      "Epoch 5, Loss: 0.38470249835933956\n",
      "Epoch 6, Loss: 0.29079146523560795\n",
      "Epoch 7, Loss: 0.21709718750346274\n",
      "Epoch 8, Loss: 0.14654573221646605\n",
      "Epoch 9, Loss: 0.09409016370773315\n",
      "Epoch 10, Loss: 0.060377017373130434\n"
     ]
    }
   ],
   "source": [
    "# Tạo hàm collate_fn tùy chỉnh để padding các tensor\n",
    "def collate_fn(batch):\n",
    "    words, tags = zip(*batch)\n",
    "    words_padded = pad_sequence(words, batch_first=True, padding_value=0)\n",
    "    tags_padded = pad_sequence(tags, batch_first=True, padding_value=-1)  # -1 cho padding\n",
    "    lengths = [len(seq) for seq in words]\n",
    "    return words_padded, tags_padded, lengths\n",
    "\n",
    "# Tạo Dataset tùy chỉnh\n",
    "class DependencyParsingDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "# Tạo DataLoader với hàm collate_fn tùy chỉnh\n",
    "train_dataset = DependencyParsingDataset(train_data)\n",
    "test_dataset = DependencyParsingDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# Xây dựng mô hình BiLSTMParser\n",
    "class BiLSTMParser(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super(BiLSTMParser, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        x = self.embedding(x)\n",
    "        packed_input = pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        output = self.fc(output)\n",
    "        return output\n",
    "\n",
    "# Khởi tạo mô hình, hàm loss và optimizer\n",
    "vocab_size = len(word_vocab)\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "output_dim = len(tag_vocab)\n",
    "\n",
    "model = BiLSTMParser(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-1)  # Bỏ qua giá trị padding\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Hàm huấn luyện mô hình\n",
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for words, tags, lengths in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(words, lengths)\n",
    "\n",
    "            # Chuyển đổi các tensor để tính toán loss chính xác\n",
    "            outputs = outputs.view(-1, output_dim)\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            loss = criterion(outputs, tags)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(train_loader)}\")\n",
    " \n",
    "# Huấn luyện mô hình\n",
    "train_model(model, train_loader, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9011de88",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Câu mẫu: Người sĩ quan tham mưu nhắc ; - đồng chí Thanh bình tĩnh nói đi .\n",
      "                          VERB(nhắc)                                    \n",
      "       _______________________|____________________________________      \n",
      " NOUN(Người)            ADJ(bình tĩnh)                             |    \n",
      "      |            ___________|___________________________         |     \n",
      "NOUN(sĩ quan)     |           |        NOUN(đồng chí) VERB(nói)    |    \n",
      "      |           |           |              |            |        |     \n",
      "VERB(tham mưu) PUNCT(;)    PUNCT(-)     PROPN(Thanh)   ADV(đi)  PUNCT(.)\n",
      "      |           |           |              |            |        |     \n",
      "     ...         ...         ...            ...          ...      ...   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vẽ cây\n",
    "import nltk\n",
    "from nltk import Tree\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def conllu_to_nltk_tree(sentence):\n",
    "    def token_to_nltk_tree(token):\n",
    "        word = token['form']\n",
    "        head = int(token['head'])\n",
    "        deprel = token['deprel']\n",
    "        upos = token['upostag']  # Thêm nhãn từ loại (POS tag)\n",
    "        return (word, upos, head, deprel)\n",
    "\n",
    "    def build_tree(tokens):\n",
    "        root = None\n",
    "        token_dict = {token['id']: token for token in tokens}\n",
    "        children = {token['id']: [] for token in tokens}\n",
    "\n",
    "        for token in tokens:\n",
    "            head_id = token['head']\n",
    "            if head_id == 0:\n",
    "                root = token['id']\n",
    "            else:\n",
    "                children[head_id].append(token['id'])\n",
    "\n",
    "        def create_tree_node(token_id):\n",
    "            token = token_dict[token_id]\n",
    "            word, upos, _, _ = token_to_nltk_tree(token)  # Lấy thông tin từ loại (POS tag)\n",
    "            subtree = [create_tree_node(child_id) for child_id in children[token_id]]\n",
    "            return Tree(upos + \"(\" + word + \")\", subtree)  # Chèn nhãn từ loại vào cây\n",
    "\n",
    "        return create_tree_node(root)\n",
    "\n",
    "    nltk_tree = build_tree([token for token in sentence])\n",
    "    return nltk_tree\n",
    "\n",
    "# Vẽ cây cho câu đầu tiên\n",
    "import random\n",
    "example_sentence = all_sentences[random.randint(1, 1000)]\n",
    "nltk_tree = conllu_to_nltk_tree(example_sentence)\n",
    "\n",
    "print(\"Câu mẫu:\", \" \" .join([token['form'] for token in example_sentence]))\n",
    "\n",
    "# Vẽ cây\n",
    "nltk_tree.pretty_print()\n",
    "plt.figure(figsize=(7,7))\n",
    "\n",
    "nltk_tree.draw()\n",
    "plt.savefig(\"dependency_tree.png\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65c6314",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
